{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a14913-2e5d-4d8a-aada-035986c59a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../cs336-basics')\n",
    "from cs336_basics.model import scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb673b67-e15c-4468-98e1-d2a5e6a5c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qkv(batch_size: int, seq_length: int, d_model: int, device: str):\n",
    "    Q = torch.randn(batch_size, seq_length, d_model, requires_grad=True, device=device)\n",
    "    K = torch.randn(batch_size, seq_length, d_model, requires_grad=True, device=device)\n",
    "    V = torch.randn(batch_size, seq_length, d_model, requires_grad=True, device=device)\n",
    "    return Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21df240f-50b5-4f25-865d-ae6b70c2f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(seq_length: int, device: str):\n",
    "    seq = torch.arange(seq_length, device=device)\n",
    "    qi = seq.view(1, seq_length, 1)                      # (1, query, 1)\n",
    "    kj = seq.view(1, 1, seq_length)                      # (1, 1, key)\n",
    "    mask = qi >= kj                             # (1, query, key) â‡’ broadcast\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a89da3-5b8f-4f0a-bc41-4a5862223b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_allocated_memory(is_gpu: bool):\n",
    "    if is_gpu:\n",
    "        return torch.cuda.memory_allocated()\n",
    "    else:\n",
    "        return torch.mps.current_allocated_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a86e448-91fc-4a81-a92b-41353ecffe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_cache(is_gpu: bool) -> None:\n",
    "    if is_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26ec1b-40ba-4668-ac7f-8950b7979489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model: 16, seq_length: 256, forward_total_time: 0.034 sec, backward_total_time: 0.080 sec, forward_max_memory: 5.2 MB, backward_max_memory: 1.0 MB\n",
      "d_model: 16, seq_length: 1024, forward_total_time: 0.121 sec, backward_total_time: 0.338 sec, forward_max_memory: 72.0 MB, backward_max_memory: 4.7 MB\n",
      "d_model: 16, seq_length: 4096, forward_total_time: 1.890 sec, backward_total_time: 6.618 sec, forward_max_memory: 1185.6 MB, backward_max_memory: 111.2 MB\n"
     ]
    }
   ],
   "source": [
    "import torch, time\n",
    "\n",
    "d_model_list = [16, 32, 64, 128]\n",
    "seq_length_list = [256, 1024, 4096, 8192, 16384]\n",
    "batch_size = 8\n",
    "is_gpu = torch.cuda.is_available()\n",
    "device = \"cuda\" if is_gpu else \"mps\"\n",
    "\n",
    "\n",
    "for d_model in d_model_list:\n",
    "    for seq_length in seq_length_list:\n",
    "        Q, K, V = create_qkv(batch_size, seq_length, d_model, device)\n",
    "        mask = create_mask(seq_length, device)\n",
    "        forward_total_time = 0\n",
    "        backward_total_time = 0\n",
    "        forward_max_memory = 0\n",
    "        backward_max_memory = 0\n",
    "        empty_cache(is_gpu)\n",
    "\n",
    "        try:\n",
    "            for _ in range(10): #warm-up\n",
    "                out = scaled_dot_product_attention(Q, K, V, mask)\n",
    "                loss = out.sum()    \n",
    "                loss.backward()\n",
    "            \n",
    "            for _ in range(100):          # warm-up & timing loop\n",
    "                forward_total_time -= time.time()\n",
    "                out = scaled_dot_product_attention(Q, K, V, mask) # forward\n",
    "                if is_gpu:\n",
    "                    torch.cuda.synchronize()\n",
    "                forward_total_time += time.time()\n",
    "                forward_max_memory = max(forward_max_memory, get_current_allocated_memory(is_gpu))\n",
    "                loss = out.sum()\n",
    "    \n",
    "                backward_total_time -= time.time()\n",
    "                loss.backward()           # backward\n",
    "                if is_gpu:\n",
    "                    torch.cuda.synchronize()\n",
    "                backward_total_time += time.time()\n",
    "                backward_max_memory = max(backward_max_memory, get_current_allocated_memory(is_gpu))\n",
    "    \n",
    "            print(f\"d_model: {d_model}, seq_length: {seq_length}, forward_total_time: {forward_total_time:.3f} sec, backward_total_time: {backward_total_time:.3f} sec, forward_max_memory: {forward_max_memory/1e6:.1f} MB, backward_max_memory: {backward_max_memory/1e6:.1f} MB\")\n",
    "        except Exception as e:\n",
    "            print(f\"d_model: {d_model}, seq_length: {seq_length}, exception: CUDA out of memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c524b-b4f5-4593-85b4-e78e94532f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
